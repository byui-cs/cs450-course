<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>CS 450 - Prove</title>
    <link rel="stylesheet" type="text/css" href="../course/style2018.css" />
</head>

<body>
    <div id="courseTitle">
        <span class="icon-byui-logo"></span>
        <h1>Machine Learning &amp; Data Mining | CS 450</h1>
    </div>
    <article>

            <h2>02 Prove : Assignment - kNN Classifier</h2>

            <h3>Objective</h3>
            <p>Understand a straightforward learning algorithm, the <em>k</em>-Nearest Neighbors algorithm.</p>

            <h3>Instructions</h3>
            <p>Add to your experiment shell from the previous assignment. Implement a new algorithm, <em>k</em>-Nearest Neighbors, that can be configured for any size of neighborhood, <em>k</em>. To classify an instance, the algorithm should identify its <em>k</em> nearest neighbors and use their labels to determine the target classification.</p>

            <p>For this assignment, you only need to run the algorithm on the Iris dataset, then next week, we will work with more interesting/complex data. Also next week, you will need to account for the fact that the different attributes have different ranges/scales for their data. This week, you can implement the algorithm without scaling/normalizing the data.</p>

            <h3>Experiment Guidelines</h3>
            <p>Because we are not using a very complex dataset this week, you don't need to do as much experimenting as you might otherwise, but you should:</p>

            <ol>
                <li><p>Experiment with different values of <em>k</em>.</p></li>
                <li><p>Compare your results to an existing implementation of <em>k</em>-Neighbors Neighbors. There are several different options for this. One option is to use the Python scikit-learn algorithm implementation. You should look up more documentation, but in short, it can be used as easily as:</p>
<pre><code class="python">
from sklearn.neighbors import KNeighborsClassifier

# ...
# ... code here to load a training and testing set
# ...

classifier = KNeighborsClassifier(n_neighbors=3)
classifier.fit(train_data, train_target)
predictions = classifier.predict(test_data)
</code></pre>

            <!-- <p>Another option is to download <a href="http://www.cs.waikato.ac.nz/ml/weka/" target="_blank">Weka</a>, which is a free, open-source Java implementation of many learning and data processing algorithms. It is not pretty or fancy, but does have a GUI that you can use to play around with these algorithms.</p>
 -->
                </li>
            </ol>
            
<!--             <h3>Questions for Consideration</h3>
            <ul>
                <li><p>How should categorical data be handled?</p></li>
                <li><p>How should missing data be handled?</p></li>
                <li><p>How should the labels of the neighbors be used (e.g., simply majority or something else)?</p></li>
            </ul>
 -->

            <h3>Requirements</h3>
            <p>As always, you are encouraged to go above and beyond and take initiative in your learning. As described in the syllabus, meeting the minimum standard requirements can qualify you for 93%, but going above and beyond is necessary to get 100%. The following are the expectations for a minimum standard, with a few suggestions for going above and beyond.</p>
            <h4>Minimum Standard Requirements</h4>
            <ol>
                <li><p>Implement basic kNN algorithm</p></li>
                <li><p>Be able to load and use the Iris dataset</p></li>
<!--                 <li><p>Appropriately handle numeric data on different scales</p></li> -->
<!--                 <li><p>Handle numeric and nominal data</p></li>
                <li><p>Normalize numeric data</p></li>
 -->
                 <li><p>Basic experimentation:</p>
                    <ul>
                        <li><p>Play with different values of K</p></li>
                        <li><p>Compare to an existing implementation</p></li>
                    </ul>
                </li>
            </ol>

            <h4>Some opportunities to go above and beyond:</h4>
            <ul>
                <li><p>KD-Tree</p></li>
                <li><p>Experiment with several more datasets</p></li>
                <li><p>Handle non-numeric data</p></li>

<!--                 <li><p>Significant Experimentation, for example:</p></li>
                <ul>
                    <li><p>Different distance metrics</p></li>
                    <li><p>Several more datasets</p></li>
                </ul>
 -->
                 <li><p>Any other ideas you have</p></li>
            </ul>


            <h3>Submission</h3>
            <p>When complete, you need to upload two things (possibly more than two files) to I-Learn:</p>
            <ol>
                <li><p>Download the <a href="prove02.txt" target="_blank">assignment submission form</a>, answer its questions and upload this form to I-Learn.</p></li>
                <li><p>Submit your source code to I-Learn. If you would prefer, you can post your source code to a Git repository and provide a link in your submission form.</p></li>
            </ol>
            
        </article>

   <script src="../course/js/highlight/highlight.pack.js"></script>
   <script>hljs.initHighlightingOnLoad();</script>


</body>

</html>