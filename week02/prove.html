<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>CSE 450 - Prove</title>
    <link rel="stylesheet" type="text/css" href="../course/style2018.css" />
</head>

<body>
    <div id="courseTitle">
        <span class="icon-byui-logo"></span>
        <h1>Machine Learning &amp; Data Mining | CSE 450</h1>
    </div>
    <article>

            <h2>02 Prove : Assignment - kNN Classifier</h2>

            <h3>Overview</h3>

            <p>Add to your experiment shell from the previous assignment. Implement a new algorithm, <em>k</em>-Nearest Neighbors, that can be configured for any size of neighborhood, <em>k</em>. To classify an instance, the algorithm should identify its <em>k</em> nearest neighbors and use their labels to determine the target classification.</p>

            <p>For this assignment, you only need to run the algorithm on the Iris dataset, then next week, we will work with more interesting/complex data. Also next week, you will need to account for the fact that the different attributes have different ranges/scales for their data. This week, you can implement the algorithm without scaling/normalizing the data.</p>

            <h4>Objectives</h4>
            <ul class="small-list">
                <li><p><a href="#item-1">Implement k-Nearest Neighbors</a>.</p></li>
                <li><p><a href="#item-2">Experiment with the algorithm parameters</a>.</p></li>
                <li><p><a href="#item-3">Verify your results with an "off-the-shelf" implementation</a>.</p></li>
            </ul>

        <h3 id="item-1">Implementation</h3>
        <p>The k-NN classifier will follow the same basic structure of the <code>HardCodedClassifier</code> classifier you implemented last week.</p>

        <ol>
            <li> <p>The <code>fit</code> method:</p>
<pre><code class="python">def fit(self, data, targets):
</code></pre>
                <p>Should store the training data and associated targets as member variables.</p>
            </li>
            <li>
                <p>You'll also need a <code>calc_distance</code> method:</p>
<pre><code class="python">def calc_distance(self, x1, x2):
</code></pre>
                <p>That takes two samples and returns the <em>Euclidean Distance</em> between those samples.</p>
                <p>Since the Iris dataset has four features (petal width (<code>PW</code>) and height (<code>PH</code>), and sepal width (<code>SW</code>) and height (<code>SH</code>)), the euclidean distance between samples <code>1</code> and <code>2</code> would be equal to:</p>
                <p>$$
                   \sqrt{(PW_{2} - PW_{1})^2 + (PH_{2} - PH_{1})^2 + (SW_{2} - SW_{1})^2 + (SH_{2} - SH_{1})^2}
                   $$
                </p>
            </li>
            <li>
                <p>Finally, your <code>predict</code> method:</p>
<pre><code class="python">def predict(self, data, k):
</code></pre>
                <p>Should take the list of $n$ test samples and a value for $k$, and return a list of $n$ predictions generated by the k-Nearest Neighbors algorithm.</p>
                <p>(i.e. if your test data had three rows, then your list of predictions should contain three predictions.</p>
                
                <p>For each sample in the test data, the <code>predict</code> method needs to use the <code>calc_distance</code> method to calculate the distance between that sample and every sample in the training data.</p>
                
                <p>Then, it should sort those distances, and determine the class that occurs the most often in the <code>k</code> closest cases. (You may find Python's <a href="https://pymotw.com/2/collections/counter.html" target="_blank">Counter module</a> useful for this step, particularly the <code>most_common</code> function.</p>
            </li>
            <li>
                <p>As with the <code>HardCodedClassifier</code>, your code should use the k-NN Classifier to generate a list of predictions for the test data, then compare those predictions with the actual classes and <a href="../week01/prove.html#accuracy">calculate the accuracy</a>.</p>
            </li>
        </ol>
       
            <h3 id="item-2">Experimentation</h3>
            <p>Because we are not using a very complex dataset this week, you don't need to do as much experimenting as you might otherwise, but you should:</p>

            <ol>
                <li><p>Experiment with different values of $k$.</p></li>
                <li><p>Compare your results to an existing implementation of <em>k</em>-Neighbors Neighbors. There are several different options for this. One option is to use the Python scikit-learn algorithm implementation. You should look up more documentation, but in short, it can be used as easily as:</p>
<pre><code class="python">
from sklearn.neighbors import KNeighborsClassifier

# ...
# ... code here to load a training and testing set
# ...

classifier = KNeighborsClassifier(n_neighbors=3)
classifier.fit(train_data, train_target)
predictions = classifier.predict(test_data)
</code></pre>

                </li>
            </ol>


            <h3>Requirements</h3>
            <p>As always, you are encouraged to go above and beyond and take initiative in your learning. As described in the syllabus, meeting the minimum standard requirements can qualify you for 93%, but going above and beyond is necessary to get 100%. The following are the expectations for a minimum standard, with a few suggestions for going above and beyond.</p>
            <h4>Minimum Standard Requirements</h4>
            <ol>
                <li><p>Implement basic kNN algorithm</p></li>
                <li><p>Be able to load and use the Iris dataset</p></li>
                <li><p>Basic experimentation:</p>
                    <ul>
                        <li><p>Play with different values of K</p></li>
                        <li><p>Compare to an existing implementation</p></li>
                    </ul>
                </li>
            </ol>

            <h4>Some opportunities to go above and beyond:</h4>
            <ul>
                <li><p>KD-Tree</p></li>
                <li><p>Experiment with several more datasets</p></li>
                <li><p>Handle non-numeric data</p></li>
                 <li><p>Any other ideas you have</p></li>
            </ul>


            <h3>Submission</h3>

            <p>When complete, you need to upload <strong>two things</strong> (possibly more than two files) to I-Learn:</p>
            <ol>
                <li><p>Download the <a href="prove02.txt" target="_blank">assignment submission form</a>, answer its questions and upload this form to I-Learn.</p></li>
                <li>
                    <p>You will then need to submit your source code.</p>

                    <p>If you used a Jupyter Notebook, you should not submit that directly. Instead, upload it to a github repository and submit a link to that file.</p> 

                    <p>If you used a Jupyter Notebook in Google Colaboratory, you can save a copy directly from there to GitHub (Click <code>File -> Save a copy in GitHub...</code></p>

                    <p>Alternatively, if you used a Jupyter Notebook using Anaconda, do not upload the .ipynb file directly, instead, please export the file to HTML (Click <code>File -> Download as... -> Html</code>). Then, upload the HTML file to I-learn.</p>

                    <p>Finally, if you used a regular <code>.py</code> source file, you can submit that (or a link to it from GitHub) directly to I-Learn. If you used Google Colaboratory, you can save the notebook as a <code>.py</code> file by clicking <code>File -> Download as .py</code></p>
                </li>
            </ol>
            
            <div id="footnotes">
                <h4>References</h4>
            
                <p id="footnote-1">1. <a target="_blank" href="http://www.comp.tmu.ac.jp/morbier/R/Fisher-1936-Ann._Eugen.pdf">Fisher, R. (1936). The use of multiple measurements in taxonomic problems. Annals Of Eugenics, 7(2), 179-188. doi:10.1111/j.1469-1809.1936.tb02137.x</a>.<a class="return href="#footnote-1-ref">&#8617;</a></p>
            </div>

        </article>

    <script src="../course/js/katex/katex.min.js"></script>
    <script src="../course/js/auto-render/auto-render.min.js"></script>
    <script src="../course/js/highlight/highlight.pack.js"></script>
    <script>
        renderMathInElement(document.body,
        {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
            ]
        });

        hljs.initHighlightingOnLoad();

        var elements = document.querySelectorAll('.language-text')

        for (var i = 0; i < elements.length; i++) {
            elements[i].classList.add('hljs');
        }
    </script>

</body>

</html>