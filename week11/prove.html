<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>CSE 450 - Prove</title>
    <link rel="stylesheet" type="text/css" href="../course/style2018.css" />
</head>

<body>
<div id="courseTitle">
    <span class="icon-byui-logo"></span>
    <h1>Machine Learning &amp; Data Mining | CSE 450</h1>
</div>
    <article>

            <h2>11 Prove : Assignment</h2>
            <p class="subtitle">Ensemble Learning</p>

            <h3>Objective</h3>
            <p>Be able to combine different machine learning algorithms in different forms of ensembles.</p>
            <h3>Instructions</h3>
            <p>This assignment is very open-ended. Your task is choose a few different datasets and apply different classification algorithms to these datasets, both individually and in different kinds of ensembles.</p>
            <p>In particular, the following are the specific baseline requirements:</p>
            <ul>
                <li><p>Select 3 different datasets of your choice.</p></li>
                <ul>
                    <li><p>Choose some that you think will be difficult to learn on, so that you can see the benefits of the ensembles play out. The <a href="https://archive.ics.uci.edu/ml/datasets.html" title="UCI Repository">UCI repository</a> is the source of many of the ones we have used this semester.</p></li>
                </ul>
                <li><p>For each dataset:</p></li>
                <ul>
                    <li><p>Try at least 3 different "regular" learning algorithms and note the results.</p></li>
                    <!-- <li><p>Use Bagging and note the results. (Play around with a few different options)</p></li> -->
                    <!-- <li><p>Use AdaBoost and note the results. (Play around with a few different options)</p></li> -->
                    <li><p>Use a random forest and note the results. (Play around with a few different options)</p></li>
                    <li><p>Use a gradient boosting machine (GBM) and note the results. (Play around with a few different options</p></li>
                </ul>
            </ul>

            <p>For more information about Gradient Boosting Machines, please read <a href="https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab" target="_blank">Understanding Gradient Boosting Machines</a>.</p>

            <h3>Languages and Libraries</h3>
            <p>You are welcome to use any language or set of libraries you like for this assignment. In Python, you can find bagging and other boosting algorithms in the <a href="http://scikit-learn.org/stable/modules/ensemble.html" target="_BLANK">sklearn-ensemble package</a>.</p>

            <p>For Random Forests and GBMs, there are a number of good options out there. One that you might consider is <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html" target="_blank">LightGBM</a> which has an sk-learn style API, which is easy to use and can be used for both random forests and GBMs by switching the <code>boosting_type</code> parameter. This is also a real-world tool that is used in industry today.</p>


<!--             
            <p>Please note that in Python, these algorithms are available in the <a href="http://scikit-learn.org/stable/modules/ensemble.html" target="_BLANK">sklearn-ensemble package</a>. In Weka, they be found in the "meta" category.</p> -->
            <p>As always, you are encouraged to go above and beyond by experimenting on several more datasets of different make-ups, as well as significant experimentation with the algorithms and their parameters.</p>
            <h3>Submission</h3>
            <p>When complete, note the experiments and accuracies in the <a href="./prove11.txt" target="_BLANK">submission form</a> and upload it to I-Learn.</p>


        </article>
    </div>


</body>

</html>